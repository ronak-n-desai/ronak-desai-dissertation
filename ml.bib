@article{Miller_1984_AmStat,
	ISSN = {00031305},
	URL = {http://www.jstor.org/stable/2683247},
	author = {Don M. Miller},
	journal = {The American Statistician},
	number = {2},
	pages = {124--126},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Reducing Transformation Bias in Curve Fitting},
	urldate = {2024-03-15},
	volume = {38},
	year = {1984}
}

@article{Pedregosa_2011_Scikit-Learn,
	title={Scikit-learn: Machine Learning in {P}ython},
	author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
	and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
	and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
	Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	journal={Journal of Machine Learning Research},
	volume={12},
	pages={2825--2830},
	year={2011}
}

@manual{Tietz_2017_Skorch,
	author       = {Marian Tietz and Thomas J. Fan and Daniel Nouri and Benjamin Bossan and {skorch Developers}},
	title        = {skorch: A scikit-learn compatible neural network library that wraps PyTorch},
	month        = jul,
	year         = 2017,
	url          = {https://skorch.readthedocs.io/en/stable/}
}

@article{Charlier_2021_KeOps,
	author  = {Benjamin Charlier and Jean Feydy and Joan Alexis Glaunès and François-David Collin and Ghislain Durif},
	title   = {Kernel Operations on the GPU, with Autodiff, without Memory Overflows},
	journal = {Journal of Machine Learning Research},
	year    = {2021},
	volume  = {22},
	number  = {74},
	pages   = {1-6},
	url     = {http://jmlr.org/papers/v22/20-275.html}
}

@misc{Wang_2019_GPytorch,
	title={Exact Gaussian Processes on a Million Data Points}, 
	author={Ke Alexander Wang and Geoff Pleiss and Jacob R. Gardner and Stephen Tyree and Kilian Q. Weinberger and Andrew Gordon Wilson},
	year={2019},
	eprint={1903.08114},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{Gardner_2018_GPytorch,
	title={GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration},
	author={Gardner, Jacob R and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q and Wilson, Andrew Gordon},
	booktitle={Advances in Neural Information Processing Systems},
	year={2018}
}

@article{Schulz_2017_ATO,
	title={A tutorial on Gaussian process regression: Modelling, exploring, and exploiting functions},
	author={Eric Schulz and Maarten Speekenbrink and Andreas Krause},
	journal={bioRxiv},
	year={2017}
}

@misc{Hensman_2014_SVGP,
	title={Scalable Variational Gaussian Process Classification}, 
	author={James Hensman and Alex Matthews and Zoubin Ghahramani},
	year={2014},
	eprint={1411.2005},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article{Wan_2019_FeatureScaling,
	doi = {10.1088/1742-6596/1213/3/032021},
	url = {https://dx.doi.org/10.1088/1742-6596/1213/3/032021},
	year = {2019},
	month = {jun},
	publisher = {IOP Publishing},
	volume = {1213},
	number = {3},
	pages = {032021},
	author = {Xing Wan},
	title = {Influence of feature scaling on convergence of gradient iterative algorithm},
	journal = {Journal of Physics: Conference Series},
	abstract = {Feature scaling is a method to unify self-variables or feature ranges in data. In data processing, it is usually used in data pre-processing. Because in the original data, the range of variables is very different. Feature scaling is a necessary step in the calculation of stochastic gradient descent. This paper takes the computer hardware data set maintained by UCI as an example, and compares the influence of normalization method and interval scaling method on the convergence of stochastic gradient descent by algorithm simulation. The result of study has a certain value on feature scaling.}
}

@Article{2020_Harris_NumPy,
	title         = {Array programming with {NumPy}},
	author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
	van der Walt and Ralf Gommers and Pauli Virtanen and David
	Cournapeau and Eric Wieser and Julian Taylor and Sebastian
	Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
	and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
	Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
	R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
	G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
	Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
	Travis E. Oliphant},
	year          = {2020},
	month         = sep,
	journal       = {Nature},
	volume        = {585},
	number        = {7825},
	pages         = {357--362},
	doi           = {10.1038/s41586-020-2649-2},
	publisher     = {Springer Science and Business Media {LLC}},
	url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@Manual{2023_RAPIDS,
	title = {RAPIDS: Libraries for End to End GPU Data Science},
	author = {RAPIDS Development Team},
	year = {2023},
	url = {https://rapids.ai}
}

@inproceedings{PyTorch_2019,
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
	volume = {32},
	year = {2019}
}

