\chapter{Machine Learning Methods Applied to Synthetic Ion Acceleration Data} \label{ch:5}

In recent years, the application of \gls{ML} methods to \gls{HEDS} has exploded due to two main reasons. First, ultra-intense laser systems are now capable of firing many shots per second and also collecting data at a similar rate. This allows scientists to collect a lot of data, which cannot reasonably be handled in real-time by a human. Second, machine learning frameworks like PyTorch \cite{PyTorch_2019} are easily accessible and powerful so that scientists who are not machine learning researchers are capable of using them.

One approach to applying machine learning with a limited amount of data is \gls{BO}. \GLS{BO}, based on the prior collected data points, attempts to find another suitable data point that is different than the prior data and expected to yield a more optimized output (according to some chosen criteria). Using \gls{BO}, Jalas et al. \cite{Jalas_2021_PRL} optimizes the quality of a laser-accelerated electron beam and Dolier et al. \cite{Dolier_2022_NJoP} optimizes the maximum proton energy using PIC simulations. Another notable effort is Loughran et al. \cite{Loughran_2023_HPLSE} who used this approach to demonstrate higher maximum cutoff proton energies from an experiment using a 1 Hz laser. While this approach is commendable, it does not scale well to laser repetition rates of 100 or more Hz. 

Quite a few works have explored using neural networks to analyze and extract information from large datasets generated from \gls{PIC} simulations. Djordevic et al. \cite{Djordjevic_2021_PPCF} used these simulations to find an empirical estimate for the effective acceleration time by using a \gls{NN} model (which informs our choice of 1.3 in \autoref{eq:fuchs_multiplier}). Schmitz et al. \cite{Schmitz_2023_LaPB} also trained \gls{NN}s on \gls{PIC} simulation data to better understand optimal laser and target parameter combinations for their system at TU Darmstadt. One thing these studies have in common is that they only use data from \gls{PIC} simulations which don't necessarily reflect real experiments (especially so because they are 1D and 2D simulations with reduced dimensionality). 

The \gls{WP-ELL} has a 1 kHz laser that can currently collect ion spectrometer at a maximum rate of 100 Hz \cite{George_2019_HPLSE}. Currently, the liquid target can be sustained for around 45 minutes before needing to be refilled which could theoretically result in 270,000 unique data points during one collection. Laser systems like this are still new and the stability of the liquid target often interferes with the goal of collecting quality data. As a result, we first focused our energy into doing a \gls{ML} analysis on data produced from a well-known model by Fuchs \cite{Fuchs_2005_Nat} based on parameters at . The goal of this analysis is not to make recommendations as to what input parameters should be used in experiment, but to provide a general framework that can be extended to real data and also to help others incorporate \gls{ML} in their work by sharing our code (see Zenodo \cite{Desai_2024_Zenodo} for the python code and datasets). By providing these files we hope to encourage others to compare other ML models against our results as a benchmark. This chapter details the work I did developing this synthetic dataset, comparing different \gls{ML} algorithms, and evaluating their potential effectiveness in a real experiment. 

\section{Modified Fuchs et. al. Model}

In this section, I will describe the model from which we generated the synthetic datasets \cite{Desai_2024_CPP,Desai_2024_arX}. First, the expansion of a plasma into a vacuum \cite{Mora_2003_PRL} is used to determine the maximum proton energy and the number of accelerated protons per unit energy $\frac{dN}{dE}$. Following Fuchs \cite{Fuchs_2005_Nat}, we introduce define the acceleration time in proportion to the pulse duration of the laser and adopt a scaling (e.g. \cref{eq:wilks}) to relate hot electron temperature to the ponderomotive potential. This, in combination with other empirical estimates, allows calculating a proton energy spectrum from up to 7 parameters: main pulse intensity, contrast, wavelength, pulse duration, target thickness, target focal position, and laser spot size.

\subsection{Plasma Expansion into a Vacuum}

This model was developed by Mora \cite{Mora_2003_PRL} in 2003 who built off of earlier efforts \cite{Crow_1975_JPP,Kishimoto_1983_PoF} in examining an isothermal expansion model. The model begins with the assumption that ions are contained in the semi-infinite interval $n_i = n_{i0}$ for $x < 0$ and no ions initially in the vacuum region for $x > 0$. The electrons are distributed according to the boltzmann relation given by \cref{eq:boltzmann} where $n_{e0} = n_e(x = -\infty)$ is the electron density in the unperturbed plasma. Through this relation, $\phi(-\infty) = 0$. The initial electron density is related to the ion density $n_{e0} = Z n_{i0}$ where $Z$ is the ion charge number for a fully ionized plasma. The potential also satisfies the Poisson equation \cref{eq:poisson} where $\rho/m = - e (n_e - Z n_i)$ is the mass density of the electrons. The solution of \cref{eq:poisson} at $t=0$ is found by integration \cite{Crow_1975_JPP} (where $E \equiv -\frac{d\phi}{dx}$) as 

\begin{equation}
	\frac{1}{2} \epsilon_0 E^2 = n_{e0} k_B T_e
	\begin{cases}
		\exp(\frac{e \phi}{k_B T_e} - 1 - \frac{e \phi}{k_B T_e}) & \mbox{if x < 0} \\
		\exp(\frac{e \phi}{k_B T_e}) & \mbox{if x > 0} \label{eq:crow_field}
	\end{cases}
\end{equation} 
From enforcing continuity of \cref{eq:crow_field} at $x=0$ (the location of the ion front initially) we determine $\phi = -k_B T_e / e$ to arrive at  

\begin{equation}
	E_{front,0} = \sqrt{\frac{2}{\exp(1)}} E_0
\end{equation}
where $E_0 \equiv \sqrt{n_{e0} k_B T_e / \epsilon_0}$. To get an estimate of the electric field at the ion front when $t > 0$ we need to consider what the characteristic time scale for ion motion is: the plasma ion frequency $\omega_{p,i}$

\begin{equation}
	\omega_{p,i} \equiv \sqrt{\frac{Z n_{e0} e^2}{m_i \epsilon_0}}	\label{eq:omegapi}
\end{equation} 
which is analogous to \cref{eq:omegape} So, in relation to the time-scale of plasma ion oscillations, a long time would refer to $\omega_{p,i} t \gg 1$. The ion fluid sound speed $c_s$ is given by 

\begin{equation}
	c_s = \sqrt{\frac{Z k_B T_e}{m_i}} \label{eq:soundspeed}
\end{equation}
and which is very similar to \cref{eq:vthermal} Using the definition of the debye length (\cref{eq:debye}) and sound speed $c_s$ we can re-express \cref{eq:omegapi} as 

\begin{equation}
	\omega_{p,i} t = \sqrt{\frac{Z k_B T_e}{m_i}} \sqrt{\frac{n_{e0} e^2}{\epsilon_0 k_B T_e}} t = (c_s t) (\lambda_{D0})
\end{equation}
where $\lambda_{D0}$ is the initial Debye length and $c_s$ is the ion sound speed. As we know from \cref{ch:2}, when $\lambda_D$ is smaller than the characteristic length scale of a system, the quasi-neutrality condition for a plasma is satisfied. In this case, the length scale would be $c_s t$ and we can show that asserting the condition $\omega_{p,i} t > 1$ is equivalent to $\lambda_D < c_s t$. We can continue by incorporating equations of continuity and the Lorentz force (\cref{eq:lorentz}) which can be expressed as 

\begin{subequations}
	\begin{align}
		\frac{\partial n_i}{\partial t} + v_i \frac{\partial n_i}{\partial x} &= - n_i \frac{\partial v_i}{\partial x} \label{eq:continuity} \\
		\frac{\partial v_i}{\partial t} + v_i \frac{\partial v_i}{\partial x} &= -\frac{Z e}{m_i} \frac{\partial V}{\partial x} \label{eq:lorentz_mora}
	\end{align}
\end{subequations}
This set of fluid equations can be solved numerically with the initial conditions for $n_i$, $E$, and $v_i = 0$, but it is more instructive to consider a ``self-similar solution'' that describes the ions moving with speed 
\begin{equation}
	v_i = c_s + x/t \label{eq:selfsimilarvelocity}
\end{equation}
for $x + c_s t > 0$. It is self-similar in the sense that the specific length and time scales are not important, only their ratio $x/t$. In this self-similar region, quasi-neutrality is maintained and the expanding electron density can be expressed as

\begin{equation}
	n_e = Z n_i = n_{e 0} \exp(-\frac{x}{c_s t} - 1) \label{eq:selfsimilardensity}
\end{equation} 
By combining \cref{eq:continuity,eq:lorentz_mora,eq:selfsimilarvelocity,eq:selfsimilardensity}, we can arrive at a solution for the self-similar electric field in this quasi-neutral region

\begin{equation}
	E_{SS} = \frac{m_i c_s}{Z e t} = \frac{k_B T_e}{e c_s t} = \frac{E_0}{\omega_{p,i} t} \label{eq:selfsimilarefield}
\end{equation}
Physically, we can interpret this as a sheet of positive charge $\sigma = \epsilon_0 E_{SS}$ at $x = - c_s t$ and a sheet of negative charge $-\sigma$ at the plasma edge. The location of this plasma edge (i.e. the location of the ion front) can be roughly obtained by equating the local Debye length $\lambda_D = \lambda_{D0} \sqrt{n_{e0}/n_e}$ to the scale length $c_s t$.

\begin{equation}
	x_{i, front} = c_s t [2 \ln(\omega_{p,i} t) - 1] \label{eq:mora_xfront}
\end{equation} 
and the ion velocity at the front can also be obtained 

\begin{equation}
	v_{i, front} = 2 c_s \ln(\omega_{p,i} t)
\end{equation}
The ion velocity can plug back into \cref{eq:lorentz_mora} to find out that $E_\text{front,SS} = 2 E_{SS}$. Mora found an approximate solution to $E_{front}$ that matches $E_\text{front,0}$ and $E_\text{front,SS}$ in their respective cases ($t = 0$ and $\omega_{p,i} t \gg 1$) as 

\begin{equation}
	E_{front} \simeq \frac{2 E_0}{\sqrt{2 \exp(1) + (\omega_{p,i} t)^2}}
\end{equation}

\begin{figure}
	\centering 
	\subfloat{
		\label{fig:fig1_mora}
		\includegraphics[width=0.5\linewidth]{planning/images/fig1_mora.PNG}
	}
	\subfloat{
		\label{fig:fig2_mora}
		\includegraphics[width=0.5\linewidth]{planning/images/fig2_mora.PNG}
	}
	\caption{The net charge density (left) as a function of position $x / c_s t$ and normalized electric field $E/E_0$ (right) for $\omega_{pi} t = 50$ taken from Fig 1 and 2 in Mora's Paper \cite{Mora_2003_PRL}. On the right, the self-similar electric field from \cref{eq:selfsimilarefield} is plotted with a dashed line.}
\end{figure}
This formula not only reaches the correct values in the limiting cases, but also effectively interpolates in the intermediary regions (i.e. $\omega_{p,i} t \sim 1$) when compared to a numerical code that solves \cref{eq:continuity,eq:lorentz_mora} without assuming a self-similar solution. In \cref{fig:fig1_mora}, we see the net charge density at some time $\omega_{pi} t = 50$ after the start of a 1D plasma expansion simulation. We can identify the $-2\sigma$ with the fastest expanding electrons and the $+\sigma$ region next to it as the positive ions getting pulled along. In \cref{fig:fig2_mora}, we can see the electric field between these two charged regions peaks $\simeq 2 E_{ss}$. Then, using this formula with $\cref{eq:lorentz_mora}$, we can determine the ion front velocity as 

\begin{equation}
	v_{i,front} = 2 c_s \ln(\tau + \sqrt{\tau^2 + 1})	
\end{equation}
where we've defined a normalized acceleration time $\tau \equiv \omega_{p,i} t / \sqrt{2 \exp(1)}$. Additionally, in the limit $\omega_{p,i} t \gg 1$, \cref{eq:mora_xfront} becomes

\begin{equation}
	x_{i, front} \simeq c_s t [2 \ln(\omega_{p,i} t) + \ln(2) - 3] \label{eq:mora_xfront_simple}
\end{equation}
The per-ion kinetic energy can now be calculated as

\begin{align}
	\mathcal{E} \equiv \frac{1}{2} m_i v_{i,front}^2 &= 2 m_i c_s^2 \ln(\tau + \sqrt{\tau^2 + 1})^2  \nonumber\\
	&= 2 Z k_B T_{e} \ln(\tau + \sqrt{\tau^2 + 1})^2 \label{eq:mora_maxE}
\end{align}
Using \cref{eq:selfsimilardensity}, we can determine the number of accelerated ions between $x = -c_s t$ and $x = x$ as

\begin{equation}
	N_i \equiv \int_{-c_s t}^{x} n_i(x') dx' = n_{i0} c_s t [1 - \exp(-\frac{x}{c_s t} - 1)]
\end{equation}
and using \cref{eq:selfsimilarvelocity}, we can show that this is equivalent to 

\begin{equation}
	N_i(x) = n_{i0} c_s t [1 - \exp(-\sqrt{\frac{2 \mathcal{E}}{\mathcal{E}_0}})] \label{eq:numprotons}
\end{equation}
where $\mathcal{E}_0 \equiv Z k_B T_e$. Now that the number of ions is expressed in terms of the energy $\mathcal{E}$, we can determine the number of accelerated ions per unit energy (per unit surface) as 

\begin{equation}
	\frac{d N}{d \mathcal{E}} = \frac{n_{i0} c_s t}{\sqrt{2 \mathcal{E} \mathcal{E}_0}} \exp(-\sqrt{\frac{2 \mathcal{E}}{\mathcal{E}_0}}) \label{eq:dNdE}
\end{equation}
	
\subsection{Modified Fuchs Model} \label{sec:fuchsv1}

When $\tau \rightarrow \infty$, \cref{eq:mora_maxE} diverges to $\infty$. This is an inherent limitation of the isothermal fluid model, and different models are able to avoid this issue \cite{Mora_2005_PRE,Passoni_2010_NJoP,Schreiber_2006_PRL}. However, a simple fix to this model involves assuming that this acceleration time is finite and proportional to the pulse duration. Physically, it makes sense that the protons are only getting accelerated on the timescale of  laser-target interactions. This is the approach taken by Fuchs \cite{Fuchs_2005_Nat} and he expresses \cref{eq:mora_maxE} as 

\begin{equation}
	E_\text{max} = 2 k_B T_h [\ln(t_p + \sqrt{t_p^2 + 1})]^2 \label{eq:fuchs_maxE}
\end{equation}
where $\tau \equiv \omega_{p,i} t_\text{acc} / \sqrt{2 \exp(1)}$ just like the Mora model. We've also set $Z=1$ to signify that we are looking for hydrogen ions (i.e. protons) The crucial difference is that we express the acceleration time as 

\begin{equation}
	t_\text{acc} \approx 1.3 \tau_\text{FWHM} \label{eq:fuchs_multiplier}
\end{equation}
One can assume that the absorption fraction of hot electrons $\eta$ (with respect to the total laser energy $E_L$) is given by $\eta_e = 1.2 \times 10^{-15} I^{0.74} \text{ W cm}^{-2}$ with a maximum of 0.5, determined from empirical scalings (e.g. see fig. 3 from Key \cite{Key_1998_PoP}). Additionally, the average energy of the hot electrons is set by the Wilks scaling \cref{eq:wilks}. Putting this together, 

\begin{equation}
	N_e = \eta_e \frac{E_L}{T_h}
\end{equation}
would be the total number of hot electrons accelerated into the target. These electrons spread out in a roughly cylindrical volume of area $S_\text{sheath}$ and length $c \tau{fwhm}$ where the circular sheath cross section can be estimated by $S_\text{sheath} = \pi (r_0 + d \tan(\theta))^2$. Here, $r_0 = w(x) \frac{\sqrt{2 \ln(2)}}{2}$ is half of the (spatial) full width at half maximum of the intensity distribution at position $x$. The effective radius of the sheath has an additional factor of $d \tan(\theta)$ where $d$ is the initial target thickness and $\theta$ is the half-angle divergence of the hot electron within the target (taken as $\theta = 25^\circ$). As a result, the hot electron number density can be expressed as 
	
\begin{equation}
	n_{e0} = \frac{N_e}{c \tau_\text{fwhm} S_\text{sheath}}
\end{equation}
With an estimate of the hot electron density, the proton spectrum can now be computed from \cref{eq:dNdE} as 

\begin{equation}
	\frac{dN}{dE} = N_0 \frac{\exp(-\sqrt{2 E/k_B T_h})}{\sqrt{2 E k_B T_h}} \label{eq:dNdE_Fuchs}
\end{equation}
where $N_0 \equiv n_{e0} c_s t_\text{acc} S_\text{sheath}$ is defined for convenience. Using a dimensionless scale for energy $\varepsilon \equiv \sqrt{2 E / k_B T_h}$, we can calculate the number of protons and total energy in protons through integrating \cref{eq:dNdE_Fuchs}

\begin{align}
	N &= N_0 (\exp(-\varepsilon_\text{min}) - \exp(-\varepsilon_\text{max})) \label{eq:fuchs_N} \\
	E_\text{tot} &= N_0 \frac{k_B T_h}{2}[\exp(-\varepsilon_\text{min})(2 + \varepsilon_\text{min}(2 + \varepsilon_\text{min})) - \exp(-\varepsilon_\text{max})(2 + \varepsilon_\text{max}(2 + \varepsilon_\text{max}))] \label{eq:fuchs_totE}
\end{align}
where $\varepsilon_\text{min} = \sqrt{2 E_\text{min} / k_B T_h}$ defines a minimum energy cutoff ($\varepsilon_\text{max}$ is analogous and chosen by \cref{eq:fuchs_maxE}). Furthermore, we can calculate the average proton energy by dividing \cref{eq:fuchs_N} from \cref{eq:fuchs_totE}

\begin{equation}
	E_\text{avg} \equiv \frac{E_\text{tot}}{N}
\end{equation}
The combination of \cref{eq:fuchs_maxE,eq:dNdE_Fuchs} have been tested across many of the early \gls{TNSA} experiments of the early 2000s for a wide range of laser intensities and pulse durations with good accuracy (see fig. 4 from Fuchs \cite{Fuchs_2005_Nat}).

\subsection{Further Model Modifications} \label{sec:fuchsv2}
When restricted to a particular laser system, the wavelength, pulse duration, and spot size are fixed. Considering the model in \cref{sec:fuchsv1}, only three adjustable parameters would be of interest -- target thickness $d$, peak intensity $I_0$ and target focal position $x$. To introduce complexity into our model, we wanted to consider the effect that a pre-expanded target would have on the proton acceleration. The pre-expansion may enhance the hot electron generation, but expansion on the rear side of the target would reduce the effectiveness of the \gls{TNSA} process. We incorporate this effect by allowing the laser to have a finite contrast $\kappa$ which relates the intensity of the main laser pulse $I_0$ to the intensity of a secondary laser pulse $I_\text{pre}$ as $I_\text{pre} = \kappa$. This pre-pulse is treated as a spike in intensity that occurs $t_0$ before the arrival of the main pulse. The pre-expanded target would have a new effective thickness given by 

\begin{equation}
	d_\text{eff} = d + 2 c_s t_0 \label{eq:d_eff}
\end{equation}
where $c_s$ is the ion sound speed from \cref{eq:soundspeed} in which the target is expanding outwards from both sides. Here $T_e$ is the temperature due to the pre-pulse and can be calculated by assuming that $T_e \propto I$ and that an intensity of $10^{12} \text{ W cm}^{-2}$ produces electron temperatures of $T_\text{pre,0} = 50$ eV. Since $n_e$ decreases as $d$ gets larger and $\omega_{p,i} \propto \sqrt{n_e}$, \cref{eq:fuchs_maxE} is inversely proportional to the target thickness. So, a larger prepulse with a longer time to expand $t_0$ will see a higher effective target thickness. Furthermore, when the target is off focus, the effective pre-pulse intensity on target is less which results in less expansion.

In addition, some of the main pulse energy can be depleted by traveling through the underdense region of this new pre-expanded target. These effects will be referred to as \emph{pump depletion} and are inspired by arguments from Decker \cite{Decker_1996_PoP}. Decker describes pump depletion as an ``etching'' process where traveling through the plasma causes wavefront edge to recede at a speed given by the ``etching velocity'' 

\begin{figure}
	\centering 
	\includegraphics[width=0.6\linewidth]{planning/images/density_profile.png}
	\caption{The electron density profile of the pre-expanded target is depicted for various times $t_0$. In this figure, $n(0) \equiv n_\text{max}$. Taken from Desai et al. \cite{Desai_2024_arX} where $z$ was used as the distance along the laser axis instead of $x$ as done in this work. }
	\label{fig:density_profile}
\end{figure}
\begin{equation}
	v_\text{etch} = (\omega_{p,e}/\omega)^2 c \label{eq:vetch}
\end{equation}
Note that this speed continuously changes throughout the exponential-scale electron density which falls off like $n \sim \exp(-x/c_s t_0)$ on both sides of the target (see \cref{fig:density_profile} for a visual). Due to conservation of particle number, if the target expands, the maximum density $n\text{max}$ will also lower and is given by $n_\text{max} = \frac{n_{e0} d}{d_\text{eff}}$. We can integrate $v_\text{etch}$ with respect to time, but it is more convenient in terms of the position since we know the range over which the under-dense plasma exists. The plasma edge $x_f$ is given by \cref{eq:mora_xfront_simple} and we will integrate up to the location of the critical density $x_0 = c_s t_0 (\ln(n_\text{max}) - \ln(n_c))$. Utilizing the change of variables $dx = c dt$ (due to the pulse traveling at the speed of light $c$), the ``etching distance'' can be calculated as \cite{Desai_2024_arX} 

\begin{equation}
	L_\text{etch} \equiv \int_{x_0}^{x_f} v_\text{etch} \frac{1}{c} dx = \frac{e^2 n_\text{max} c_s t_0}{\epsilon_0 m_e \omega^2} \left( \exp{\left(-\frac{x_0}{c_s t_0}\right)} - \exp{\left(-\frac{x_f}{c_s t_0}\right)} \right)
\end{equation}
Finally, this etching reduces the effective pulse duration by 

\begin{figure}
	\centering 
	\includegraphics[width=0.6\linewidth]{planning/images/energy_dip_morrison.png}
	\caption{The dotted black line shows the maximum proton energy predicted by \cref{eq:fuchs_maxE} with the pump depletion considerations in \cref{sec:fuchsv2} assuming $t_0 = \SI{60}{\pico \second}$, $I_0 = 10^{19} \text{W cm}^{-2}$, $\kappa=10^{-7}$, $d=\SI{0.5}{\micro \meter}$. The red stars indicate the predicted positions of maximum proton energy $\sim \SI{12}{\micro \meter}$. This plot is overlayed on top of an experimental maximum proton energy distribution from Morrison et. al. \cite{Morrison_2018_NJoP}. This figure is taken from Desai et. al. \cite{Desai_2024_arX}.}
	\label{fig:energy_dip_morrison}
\end{figure}
\begin{equation}
	\tau_\text{fwhm,eff} = \tau_\text{fwhm} (1 - \frac{L_\text{etch}}{c \tau_\text{fwhm}}) \label{eq:tau_etch}
\end{equation}
This model, however, is not without its flaws. First, our calculations assume a critical density of a tenth of the amount of the actual critical density. Second, instead of defining $d_\text{eff} = d_0 + 2 x_0$ (which would be the true effective density that remains above critical density), we substitute it with \cref{eq:d_eff}. Third, we modify the multiplier seen in \cref{eq:fuchs_multiplier} from 1.3 to 25 which is a significant departure. Finally, the proportionality $T_e \propto I$ with $T_\text{pre,0} = 50$ eV is chosen arbitrarily. Despite these drawbacks, we obtain model predictions similar to what is seen in \cref{fig:energy_dip_morrison} to account for the maximum proton energy dip at peak focus. The goal of creating this model modification is to add complexity to the underlying physics for the purposes of evaluating the effectiveness of machine learning models, not to invent new physics. 

\section{Results}

In this section, the results from two efforts will be analyzed. The first effort was published in \emph{Contributions to Plasma Physics} \cite{Desai_2024_CPP} in November of 2024 and the second is currently under consideration at \emph{APL Machine Learning}.

\subsection{First Analysis}

\subsubsection{Methods}
Our first analysis explored a 25,000 point dataset (described in \autoref{sec:fuchsv1}) which had three output quantities: $E_\text{max}, E_\text{tot}, E_\text{avg}$. Three input quantities were varied: peak laser intensity $I_0$ (from  $10^{18} \text{W cm}^{-2}$ to $10^{19} \text{W cm}^{-2}$), target thickness $d$ (from $\SI{0.5}{\micro \meter}$ to $\SI{5}{\micro \meter}$), and distance from peak focus to the target $z$ (from $\SI{-30}{\micro \meter}$ to $\SI{30}{\micro \meter}$). The input quantities were randomly chosen (uniformly within their intervals) with the Fuchs et al. model evaluations yielding the three outputs. To make the problem non-trivial, noise was added to the outputs sampled from a log-normal distribution with a mean of the output value and standard deviation between 0 and 30 \% of the output value. Varying the noise, which would be present in an experimental dataset, will show how well a model can handle noise. Since we define the standard deviation as proportional to the mean (rather than some constant), the raw amount of noise becomes larger as the predictions get larger.

As a pre-processing step, we first applied a logarithm to both the intensity and three outputs of the model. This was necessary due to our choice of sampling the intensity uniformly with respect to the exponent (i.e. 18-19) and the output energies are directly proportional to the intensity. Additionally, we applied z-score normalization to both the inputs and outputs which is standard in machine learning algorithms to keep all data points on the same order of magnitude. From the 25,000 total points, we used 20,000 for training and reserved a hold-out set of 5,000 for testing. We determined the scalings from the training set only to ensure there is no leakage of information into the testing set. Furthermore, we multiplied the unscaled outputs by a correction factor equal to the mean of the training outputs divided by the unscaled outputs to reduce and under-predicting bias introduced by the log-scaling of the outputs \cite{Miller_1984_AmStat}.

We used three different \gls{ML} models in our study: \gls{SVR}, \gls{GPR}, \gls{NN} that were programmed using RAPIDS \cite{2023_RAPIDS}, GPyTorch \cite{Gardner_2018_GPytorch}, and Skorch \cite{Tietz_2017_Skorch} respectively in Python. These packages all contain functions to facilitate running \gls{GPU} accelerated scripts and were ran using the Pitzer cluster on the \gls{OSC}. All three approaches benefitted from hyperparameter optimization through the \texttt{GridSearchCV} module of scikit-learn. These choices are summarized in \autoref{tab:hps1}. For both the \gls{GPR} and \gls{SVR}, a \gls{RBF} kernel was used.

\begin{table}
	\centering
	\begin{tabular}{|c|c|}
			\hline
			NN & BS = 256, leaky ReLU, $N_h = 3$, $N_l = 64$, Adam, $\text{LR}=0.001$ \\
			\hline
			SVR & $\epsilon = 0.01$, $C = 2.5$, tolerance = $0.001$ \\
			\hline
			GPR & Iterations = 30, $\text{LR}=0.001$ \\
			\hline
	\end{tabular}
	\caption{Average GPU memory consumption results when training on data with 20000 points.}
	\label{tab:hps1}
\end{table}

\subsubsection{Results}

Our first task was to evaluate how well the trained models can fit the underlying dataset as the number of data points is increased. We accomplished this by evaluating the models on a testing set which is which is proportional to the amount of training points in a 80-20 training-testing split. In \autoref{fig:mape_3levels}, the \gls{MAPE} is evaluated between the model outputs and the testing data outputs to assess the accuracy of each method for three different noise levels -- 0, 15, and 30 \%. Since, in this figure, the testing data contains noise, it is impossible to achieve 0 \% error. As a result, we've included a black line to show the error between the testing data and the theoretical value predicted by the Fuchs et al. model. We can clearly see that the all the \gls{ML} models don't appear to get better when we increase the number of data points.  Additionally, we can see that the \gls{NN} model has the worst accuracy and and more variability.

\begin{figure}
	\centering 
	\includegraphics[width=0.75\linewidth]{planning/images/paper1/test_mape_3levels.eps}
	\caption{\gls{MAPE} versus number of training points from \gls{ML} model predictions for (a) max proton energy, (b) total proton energy, (c) average proton energy and noisy testing data. Each panel shows results from (solid) 0\%, (dashed) 15\% and (dotted) 30\% added noise in the data. Black lines with different line types indicate the \gls{MAPE} between the noisy and noiseless data. Because we only compare \gls{ML} models to noisy data in this figure, these black lines indicate the best that any \gls{ML} model could conceivably do. Figure and caption taken from Figure 3 of Desai et al. \cite{Desai_2024_CPP}.}
	\label{fig:mape_3levels}
\end{figure}

Then, we kept the number of data points fixed at 2000 and varied the noise level from 0 to 30 \% and made a similar analysis in \autoref{fig:mape_noise}. These plots show how the \gls{NN} model doesn't do well without any noise. In addition to the noisy test data (solid line), the error with the same data without noise was also plotted as a dashed line. One interesting feature about this plot is that the \gls{NN} model seems more resilient to noise in the sense that it does not increase very much as the noise level increases. 

\begin{figure}
	\centering 
	\includegraphics[width=0.75\linewidth]{planning/images/paper1/test_mape_points=2.0k.eps}
	\caption{Solid lines show the typical \gls{MAPE} in (a) maximum proton energy, (b) total proton energy, and (c) average proton energy when the \gls{ML} models (which were trained on 2000 synthetic data points with noise) are evaluated on data with different levels of noise. Dashed lines show the typical error when those same ML models are evaluated on noiseless test data. Black solid lines indicate the \gls{MAPE} between the noisy and noiseless data. Figure and caption taken from Figure 4 of Desai et al. \cite{Desai_2024_CPP}.}
	\label{fig:mape_noise}
\end{figure}

Next, we compared the execution time of the different \gls{ML} and found that the \gls{SVR} runs the fastest, the \gls{GPR} runs the slowest, and the \gls{NN} runs at a speed somewhere in between. The \gls{GPR} scales roughly as $O(N^3)$ which can be contrasted with the \gls{NN}s $O(N)$. In terms of GPU memory utilized, the \gls{GPR} used the most at around 14 GB, whereas the \gls{SVR} and \gls{NN} used between 1 and 2 GB. 

\begin{figure}
	\centering 
	\includegraphics[width=0.6\linewidth]{planning/images/paper1/time.eps}
	\caption{Comparing the execution time of the different \gls{ML} models averaged across noise levels in computing the maximum, total, and average proton energies. Figure and caption taken from Figure 5 of Desai et al. \cite{Desai_2024_CPP}.}
	\label{fig:execution_time}
\end{figure}

\begin{figure}
	\centering 
	\includegraphics[width=0.75\linewidth]{planning/images/paper1/fuchs_optim.pdf}
	\caption{Parameters that produce maximum proton energy cutoffs in three different desired ranges: 1.0~MeV, 0.5~MeV and 0.25~MeV. Combinations of thickness and focal distance that produce these energy cutoffs (irrespective of the laser to proton conversion efficiency) are shown with dotted red lines. With each red line we also show with dotted gray lines the thicknesses and focal distances that produce proton energy cutoffs that are +15\% or -15\% of the cutoff goal. Green shaded areas show regions where the laser to proton conversion efficiency is high (i.e. within 5\% of the optimal value). A green star shows the ideal conditions for maximizing the proton conversion efficiency. The blue region corresponds to using all the terms in \autoref{eq:fuchsv1_function} and the blue star indicates the ideal conditions according to that minimization scheme. Figure and caption taken from Figure 6 of Desai et al. \cite{Desai_2024_CPP}.}
	\label{fig:banana}
\end{figure}

We highlighted a useful application of these models by designing an optimization task -- to determine a set of inputs that would produce a proton energy spectrum up to a specified maximum energy. Additionally, these inputs would correspond to a high number of proton in that energy range as well. We characterize the balance of these two quantities -- the maximum cutoff energy $KE_\text{cutoff}$ and the laser to proton conversion efficiency $\eta_\text{proton}$ -- as 

\begin{equation}
	f(KE_{\rm cutoff},\eta_{proton}) = \frac{|KE_{\rm cutoff} - KE_{\rm cutoff,goal}|}{KE_{\rm cutoff, goal}} + \frac{C}{\eta_{proton}} + g( KE_{\rm cutoff},KE_{\rm cutoff,goal})  \\ \label{eq:fuchsv1_function}
\end{equation}

where $C$ is a parameter that can influence the relative strength of $\eta_\text{proton}$ in comparison to $KE_\text{cutoff}$. These features can seen in \autoref{fig:banana} where three different energy cutoffs were chosen to optimize towards: 1, 0.5, and 0.25 MeV. The green region focuses on regions of the parameter space that closely match the given cutoff, while the blue region additionally factors in the $\eta_\text{proton}$. For practical reasons, we impose a penalty term $g(KE_\text{cutoff}, KE_\text{cutoff, goal})$ that prevents choosing points that have $KE\text{cutoff}$ more than 15 \% of the specified cutoff. These plots are generated from the Fuchs et al. model with 0 added noise, so they should be regarded as the ideal results that can be compared with the \gls{ML} models.

\begin{figure}
	\centering 
	\includegraphics[width=0.95\linewidth]{planning/images/paper1/model_optim_noise=30_pts=2000.pdf}
	\caption{Parameters that produce maximum proton energy cutoffs according to three trained ML Models on 2000 data points with 30 $\%$ added noise: (a) GPR, (b) SVR, (c) NN compared against the red and gray lines plotted in Fig.\ref{fig:banana}. The green and blue shaded regions are scatter plots of a subset of evaluated points that fall within 5 $\%$ of the model's predicted optimum according to the same criteria in Fig. \ref{fig:banana}. Figure and caption taken from Figure 7 of Desai et al. \cite{Desai_2024_CPP}.}
	\label{fig:banana3}
\end{figure}

We generated the same plots as \autoref{fig:banana} with trained \gls{ML} models on 2000 training data points with 30 \% added noise which can be seen in \autoref{fig:banana3}. These results show the \gls{GPR} matching the closest to \autoref{fig:banana} and the \gls{SVR} performs almost as well. On the other hand, the \gls{NN} clearly has more erratic shapes which is not surprising due to its lower accuracy from the preceding analysis in \autoref{fig:mape_noise}. 

\subsubsection{Conclusion}

We tested three different machine learning models on a synthetic data set for laser-accelerated protons by training these models on up to 20000 synthetic data points. The data set was generated using a modified Fuchs et al. \cite{Fuchs_2005_Nat} model that included Gaussian noise to simulate the kind of noise that could be present in a real experiment. The machine learning models were \acrfull{GPR}, \acrfull{SVR} and a three-hidden-layer \acrfull{NN}. 

Of the three methods, the \gls{NN} models were the least accurate and this was especially true when the number of training points was low. This included an exercise where we used the \gls{ML} models to predict optimum conditions for proton acceleration at different cutoff energies. The poor accuracy of the \gls{NN} model likely stems from the large number of free parameters that must be precisely determined for the model to become highly accurate.

In terms of performance on one \gls{GPU}, \gls{SVR} was by far the fastest model for execution time. \gls{GPR} was the slowest, taking 30 seconds to train on 20000 data points. The neural network used the least amount of \gls{GPU} memory while \gls{GPR} consumed 5-10 times more \gls{GPU} memory than the other models. Generally the performance results suggest that quasi-real time training of these models on kHz repetition rate laser systems should be feasible, especially with \gls{SVR}.

We stress that these results should not be regarded as the final word on the usefulness of these ML models in the context of laser acceleration of protons. More complex data sets may yield different results. Moreover, there are certainly ML models that deserve attention that we did not study here. 

\subsection{Second Analysis}

\section{Discussion}