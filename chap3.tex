\chapter{Computational Methods} \label{ch:3}

\section{The Particle-In-Cell Method}

The \gls{PIC} method involves solving Maxwell's Equations on a grid 

\begin{align}
	\nabla \cdot \vec{E} &= \frac{\rho}{\epsilon_0}  \label{eq:gauss} \\
	\nabla \times \vec{E} &= - \frac{\partial \vec{B}}{\partial t} \label{eq:faraday} \\
	\nabla \cdot \vec{B} &= 0 \label{eq:gauss_magnetism} \\
	\nabla \times \vec{B} &= \mu_0 (\vec{J} + \epsilon_0 \frac{\partial \vec{E}}{\partial t}) \label{eq:ampere}
\end{align}
This is combined with the lorentz force

\begin{equation}
	\vec{F} = q(\vec{E} + \vec{v} \times \vec{B}) \label{eq:lorentz_pic}
\end{equation}
which determines the motions (i.e. $\vec{r}$ and $\vec{v}$) of charged particles by integration. It is impossible to keep track of the true numbers of particles in this type of simulation which would be roughly on the order of Avogadro's number $\sim 10^{23}$. Instead, we lump many particles together into what is called a \emph{macro particle}. For example, one ``macro electron'' could contain 1 trillion ``real electrons''. Also, we cannot hope to have infinite precision in calculating quantities of interest. Spatially, we must separate the simulation volume into a grid where each cell has length $\Delta x$, $\Delta y$, and $\Delta z$ in the x, y, and z direction respectively. Temporally, we introduce a time step $\Delta t$ which allows us to propagate Maxwell's Equations forward in time by $\Delta t$ for every iteration.

For simplicity, non-relativistic equations will be introduced in this section, but they can easily be generalized to the relativistic versions which are implemented in modern PIC codes. Additionally, some of the equations will assume a 2D grid, but a 3D grid is similarly straightforward to generalize.

\subsection{Densities and Shape Factors}

When a simulation is initialized, all the particles will have a defined position and velocity. The charge density $\rho_{i,j}$ (for the cell at the $i^\text{th}$ and $j^\text{th}$ grid point in the x and y directions) is easy to compute -- it is simply the sum of all the charges $q_\alpha$ closest to grid point $(i,j)$ divided by the cell area: $\rho_{i,j} \equiv \frac{\sum_\alpha q_\alpha}{\Delta x \Delta y}$ (in 2D symmetry we additionally divide by 1 meter in the z direction to get the units right). The current density $\vec{J}_{i,j}$ can be obtained similarly -- $\vec{J}_{i,j} \equiv \frac{\sum_\alpha q_\alpha \vec{v}_\alpha}{\Delta x \Delta y}$. Assigning the densities to the nearest grid point in this manner is sensibly called \gls{NGP} by Birdsall and Langdon \cite{Birdsall_2004_PIC}.

Since the PIC approach contains many real particles in each macro particle, it is desired to smooth the macro particle densities throughout the cell(s). We can modify the individual density contributions of particles by a shape factor $S(\vec{r_\alpha} - \vec{r})$ that depends on a particle's location $\vec{r_\alpha}$ in relation to a grid point located at $\vec{r}$. This shape factor is normalized so that integrating it over the area of the simulation yields 1 to ensure the particle number is properly being conserved. The simplest improvement over \gls{NGP} would be the \emph{top hat} shape factor (also called Cloud in Cell \cite{Birdsall_2004_PIC}) which assigns density contributions proportional to proximity of the nearest cells within ($\Delta x$,$\Delta y$). This has the shape of a uniform distribution and thus looks like a ``top hat'' in 1D. It is a $0^\text{th}$ order shape factor and can be represented by the following equation

\begin{equation}
	S_0(x) \equiv \begin{cases}
		1 & \text{if } \lvert x \rvert \leq 0.5 \Delta x \\
		0 & \text{otherwise}
	\end{cases} \label{eq:tophat}
\end{equation}
A further improvement will weigh the particles closer to a particular grid point higher than a particle further away. If this weighting is linear over an area ($2 \Delta x$, $2 \Delta y$), it is called the \emph{triangle} shape factor and reprsented by the following equation in 1D

\begin{equation}
	S_1(x) \equiv \begin{cases}
		1-\frac{\lvert x \rvert}{\Delta x} & \text{if } \lvert x \rvert \leq \Delta x \\
		0 & \text{otherwise}
	\end{cases} \label{eq:triangle}
\end{equation}
It turns out that the higher order shape factors $S_n(x)$ can be represented by convolutions of $S_0(x)$ 

\begin{figure}
	\centering 
	\includegraphics[width=0.6\linewidth]{planning/images/shape_functions.png}
	\caption{The top hat ($S_0(x)$), triangle ($S_1(x)$) and $3^\text{rd}$ order spline ($S_3(x)$) are plotted in 1D.}
	\label{fig:shape_factors}
\end{figure}

\begin{equation}
	S_n(x) \equiv \int_{-\infty}^\infty S_{n-1}(x')S_0(x-x') dx'
\end{equation}
and the shape factors for $n \geq 2$ are commonly called n-splines. The third order spline is used in this work and weights particles over an area ($4 \Delta x$, $4 \Delta y$) and is represented in 1D by 

\begin{equation}
	b_3(x) = \begin{cases}
		\frac{1}{6}(8 - 12 \lvert \tilde{x} \rvert + 6 \tilde{x}^2 - \tilde{x}^3) & \text{if } 1 \leq \lvert \tilde{x} \rvert \leq 2 \\
		\frac{1}{6}(4 - 6 \tilde{x}^2 + 3 \tilde{x}^3) & \text{if } \lvert \tilde{x} \rvert \leq 1 \\
		0 & \text{otherwise}
	\end{cases}
\end{equation}
where $\tilde{x} \equiv x / \Delta x$ normalizes the position $x$. See \cref{fig:shape_factors} for a comparison of the three shape factors. 

These shape factors not only apply to the calculation of densities, but also to the electric and magnetic fields. In this way, the fields used to update particle positions and velocities are averaged over neighboring cells.

\subsection{Field Solver and Particle Push}

\begin{figure}
	\centering 
	\includegraphics[width=0.8\linewidth]{planning/images/yee_grid.PNG}
	\caption{The ``Yee'' grid is depicted (left) where the electric and magnetic field components are staggered by half a cell. The fields, currents, position, and velocity make use of the staggered grid by leapfrog time integration (right). This picture was taken from the WarpX documentation}
	\label{fig:yee_grid}
\end{figure}

The \gls{PIC} method is able to make efficient use of the second order accurate central difference approximation to compute derivatives. A simpler method like Euler integration is only first order accurate and will suffer in terms of accuracy. Higher order methods like 4th order Runge-Kutta have much higher computational costs in terms of operations per time step and memory consumption. The central difference scheme is accomplished by alternately calculating electric and magnetic fields, staggered by half a time step, in an approach called \emph{leapfrog integration} \cite{Birdsall_2004_PIC}. This can be seen in the right half of \cref{fig:yee_grid} where the calculations of $E$ and $J,B$ alternate in a ``leapfrog'' fashion. It turns out that this staggering also comes with some nice properties like automatically satisfying \cref{eq:gauss_magnetism}. By rearranging \cref{eq:faraday}, we can update the electric and magnetic fields through the following equations \cite{Arber_2015_PPCF}

\begin{align}
	\vec{E}^{n+1} &= \vec{E}^{n} + \Delta t (c^2 \nabla \times \vec{B}^{n+\frac{1}{2}} - \frac{1}{\epsilon_0} \vec{J}^{n+\frac{1}{2}}) \label{eq:E_update} \\
	\vec{B}^{n+\frac{1}{2}} &= \vec{B}^{n-\frac{1}{2}} - \Delta t (\nabla \times \vec{E}^{n}) \label{eq:B_update}
\end{align}
where $\vec{J}^{n+\frac{1}{2}} \equiv \frac{\sum_\alpha q_\alpha \vec{v}^{n+\frac{1}{2}}_\alpha}{\Delta x \Delta y}$ depends on the velocity. The updated velocity for each particle is calculated through the force from \cref{eq:lorentz_pic}. 

\begin{equation}
	\frac{v^{n + \frac{1}{2}}_\alpha - v^{n - \frac{1}{2}}_\alpha}{\Delta t} = \frac{q}{m}[E^n_\alpha + \frac{v^{n + \frac{1}{2}}_\alpha + v^{n + \frac{1}{2}}_\alpha}{2} \times B^n_\alpha] \label{eq:v_update}
\end{equation}
The $\alpha$ subscript indicates the quantities are calculated for each particle; thus, the fields are smoothed out by the shape factor (e.g. $E^n_\alpha \equiv \int E^n S_3(x-x_\alpha, y-y_\alpha, z-z_\alpha) dx \; dy \; dz$). In practice, \cref{eq:B_update,eq:E_update} are broken up into half-steps so that the electric and magnetic field are known for all half-steps. At first glance, \cref{eq:v_update} does not appear to have an explicit solution for $v^{n + \frac{1}{2}}$. There are implicit methods that can solve this equation which are used in codes like \gls{LSP} \cite{Welch_2004_LSP}. It turns out that there is an explicit solution given by the \emph{Boris Rotation Algorithm}. If we define 

\begin{align}
	v^{n + \frac{1}{2}} &= v^+ + \frac{q E^n}{2 m} \Delta t \label{eq:vplus} \\
	v^{n - \frac{1}{2}} &= v^- - \frac{q E^n}{2 m} \Delta t \label{eq:vminus}
\end{align}
we can separate out the electric field dependence to get 

\begin{equation}
	\frac{v^+ - v^-}{\Delta t} = \frac{q}{m}[\frac{v^+ + v^-}{2} \times B]
\end{equation}
which can conveniently be calculated through a rotation \cite{Birdsall_2004_PIC} through the following steps:

\begin{enumerate}
	\item Compute $v^-$ from \cref{eq:vminus}.
	\item Compute $\vec{t} \equiv \frac{q \Delta t}{2 m} \vec{B}^n $ (equivalent in magnitude to $\tan(\theta/2)$ where $\theta$ is the rotation angle)
	\item Compute $\vec{s} = \frac{2 \vec{t}}{1 + t^2}$ (equivalent in magnitude to $\sin(\theta/2)$)
	\item Compute $\vec{v}' = \vec{v}^- + \vec{v}^- \times \vec{t}$.
	\item Compute $\vec{v}^{n+\frac{1}{2}}$ from \cref{eq:vplus}.
\end{enumerate}
Now, the particles can be advanced or ``pushed'' from

\begin{equation}
	x^{n+1} = x^{n} + v^{n+\frac{1}{2}} \Delta t \label{eq:particlepush}
\end{equation}
For completeness, the velocity initially needs to be pushed backwards from $v^{0} \rightarrow v^{-\frac{1}{2}}$. This is not done in a time-centered way, but is only needed at the start of the simulation. 

\section{Machine Learning}
What is \gls{ML}? According to the computer scientist Arthur Samuel who popularized the term in 1959, it is the 

\begin{quote}
	... field of study that gives computers the ability to learn without being explicitly programmed. \cite{Geron_2023_ML}
\end{quote}

The book \emph{Hands-On Machine Learning with Scikit-Learn} by Aurelien Geron \cite{Geron_2023_ML} gives a simple example of how a \gls{ML} model differs from a more traditional computer program in the form of an email spam filter. A traditional filter would implement user specified rules that mark emails as spam based on e-mail address and subject line keywords for example. A \gls{ML} approach would determine what keywords and e-mail addresses to look for based on prior emails that have been flagged as spam.

The key difference in the two approaches is predictive power. The traditional filter can remove spam from messages that fit a pattern explicitly given by the user. The \gls{ML} filter can learn a generalized model that can predict possibly unforeseen types of spam. Since the \gls{ML} model is much more general, it is more susceptible to incorrect predictions. In this section, we'll overview how to handle data for \gls{ML} algorithms, compare model training for different methods, and explain how these models can be used in practice. The information below is largely from the book by Geron \cite{Geron_2023_ML} unless specified otherwise.

\subsection{Pre-processing}
\gls{ML} can be either supervised or unsupervised based on whether the data is labeled with a particular output. In the \gls{ML} context, model outputs are often referred to as labels and model inputs are often referred to as features. For the e-mail classification example, a supervised model would have training data that is already labeled as spam (or not) and an unsupervised model would have to infer if the data is spam by other means (e.g. clustering similar emails together). We will not be considering unsupervised learning in this work because we are ultimately interested in labeled data (which would be electron or proton energy spectra). To train an \gls{ML} effectively, there are various things we need to take into account: 

\begin{itemize}
	\item Data Quantity: We need a sufficient amount of data in order to learn relevant trends and patterns. For example, a good spam classifier would have many e-mails (thousands to millions) to train on.
	\item Representative Data: The data should be representative of all data points. A spam classifier trained only on data prior to 2010 will not accurately predict modern spam emails.
	\item Poor-Quality: The data should not have significant errors, outliers, and noise. 
	\item Irrelevant Features: The input data (features) to the model should all be relevant. For example, taking into account the exact time that a spam e-mail is received may not be the most relevant as spam emails are often sent out at random times. 
\end{itemize}
Failure to take the preceding steps into account can oftentimes lead to one of two things

\begin{itemize}
	\item Overfitting: This can happen when the data is too complex in relation to the data. An example would be using a polynomial to fit a set of points that fall on a straight line. To fix this, we can constrain the model to make it more simple, gather more training data, or reduce noise in the training set.
	\item Underfitting: When the model is too simple in relation to the data. This can be fixed by selecting a more complex model, using better features, and reducing any constraints on the model.
\end{itemize} 
To improve the quality of the existing data, we can perform \emph{pre-processing} which is the set of actions performed to transform raw data into data ready to be fed into a \gls{ML} model. In \autoref{ch:6}, significant pre-processing was needed which included removing missing values, combining data sources, applying a median-based filter, compressing the spectra into two metrics, and more. One particularly effective method of pre-processing that is specific to \gls{ML} is data normalization which makes sure that all the inputs are approximately on the same scale. The two most common ways to approach this are summarized below: 

\begin{itemize}
	\item Standard Scaling: A particular feature can be transformed through the following: $x_i \rightarrow \frac{x_i - \mu}{\sigma}$ where $\mu$ is the mean of all the data points $x_i$ and $\sigma$ is the standard deviation. This normalizes the data to be centered at 0 and have a standard deviation of 1. 
	\item Min Max Scaling: A particular features can be transformed by setting the minimum value to 0, the maximum value to 1, and all values in between would be linearly proportional. 
\end{itemize}
The Min Max Scaling is useful when the data is known to be uniformly distributed with a fixed minimum and maximum. The Standard Scaling is more general and works best if the data is normally distributed. 

\subsection{Model Selection}
Given a pre-processed dataset, some additional steps are needed before deploying a \gls{ML} model. There are many different types of models one can select and even a particular model can have different so-called \emph{hyperparameters} that characterize it. For example, a hyperparameter of the polynomial regression model is the degree. A standard approach to model selection is through \emph{cross-validation} which breaks up a dataset into different partitions and varies which ones are used for training and which are used for model evaluation. If a model has good performance with one partition but not with another, then the model would not generalize well to make good predictions. In the \emph{cross-validation} process we only want to use data from a specified training set and not from a different testing set that we want to make useful predictions on. A typical training-testing split is 80-20\% (which is what was used in \autoref{ch:5}). 

To implement cross-validation, we pick a number of partition arrangements for the training set and evaluate the model performance for each partition for a number of hyperparameter combinations. For the polynomial regression example, we could evaluate the average error between 5 partition arrangements for polynomial degree ranging from 2 to 7 and pick the model that has the lowest amount of error. In python, we use the \texttt{GridSearchCV} module of scikit-learn \cite{Pedregosa_2011_Scikit-Learn} package to handle cross-validation for a ``grid'' of hyperparameter combinations.

To evaluate model performance, one needs to choose an error (or accuracy) metric. Two common error metrics are the \gls{MSE} and the \gls{MAPE} which are defined as follows: 

\begin{align}
	\text{MSE} &\equiv  \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2 \\
	\text{MAPE} &\equiv \frac{1}{N} \sum{i=1}^N \lvert \frac{y_i - \hat{y}_i}{y_i} \rvert
\end{align}
where $y_i$ is a particular output value, $\hat{y}_i$ is the \gls{ML} model prediction, and $N$ is the number of points in the dataset. By defining these error metrics, the model with the lowest cross-validation error should be the best model. 

\subsection{Model Types}

A polynomial model is one simple model that involves minimizing the \gls{MSE} between data values and model predictions of the following formula: 

\begin{equation}
	\hat{y} = \beta_{0,0} + \beta_{1,0} x_1 + \beta_{0,1} x_2 + \beta_{1,1} x_1 x_2 + \cdots
\end{equation}
where $\beta_{i,j}$ are the polynomial coefficients. This equation incorporated two inputs $x_1$ and $x_2$ but can be generalized for any number of inputs. These coefficients can be found by a least-squares minimization algorithm. 

In principle, any dataset can be fit by a polynomial model of a sufficiently high degree. However, such a model would be prone to overfitting -- a good model with predictive power should be able to predict data not yet seen. Wolpert \cite{Wolpert_1997_IEEE} famously showed in his 1996 paper that if we don't make any assumptions about the data at hand, then there is no reason to prefer a particular \gls{ML} model over another. This is sometimes referred to as the ``no free lunch'' theorem. Therefore, it is always instructive to experiment with a variety of models to see which one works best. 

Instead of a polynomial, more sophisticated regression models can be used like the ridge regression which uses a regularization hyperparameter to avoid overfitting. In a similar vein, \gls{SVR} (which falls in the class of support vector machines) use a regularization parameter that tries to maximize the number of data points that are close to the prediction but do not exactly match the prediction. 

A big advantage of the \gls{SVR} is that it can take advantage of the so-called ``kernel trick'' which allows the model to use higher dimensional features (like the polynomial model), but without explicitly calculating all the higher order components. Importantly, this has both time and memory saving implications. A common ``kernel'' is the \gls{RBF} which calculates a similarity measure between two data points involving a dot product which is easy to compute. For more information on support vector machines, see the text by Geron \cite{Geron_2023_ML}. 

Another regression method that can make use of the kernel trick is the Gaussian Process -- a method that doesn't just construct one function to fit the data, but rather a distribution of functions that all fit the data. The functions can effectively be averaged to create a \gls{GPR} or an uncertainty can be quantified by taking a standard deviation of this probability distribution. 

A more general purpose model capable of learning functions of extremely high complexity is the \gls{NN}. This model, just like a linear regression, fits an intercept and coefficients to each input variable. Unlike the linear regression, the output of the first linear regression is the input of a 
